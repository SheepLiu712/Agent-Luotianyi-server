from ..llm.llm_module import LLMModule
from ..llm.prompt_manager import PromptManager
from typing import Dict, Any, List, Optional
from jinja2 import Template
import time
import dataclasses
import json
from typing import Tuple, Dict, List 
from ..types.tool_type import MyTool
from ..utils.logger import get_logger

@dataclasses.dataclass
class PlanningStep:
    action: str
    description: str
    parameters: Dict[str, Any]


class Planner:
    def __init__(
        self, config: Dict[str, Any], prompt_manager: PromptManager
    ) -> None:
        self.logger = get_logger(__name__)
        self.config = config
        self.llm = LLMModule(config["llm_module"], prompt_manager)
        self.variables: List[str] = self.llm.prompt_template.get_variables()
        self.all_tools: Dict[str, MyTool] = {}
        self._init_static_variables_sync()

    def register_tools(self, tools: Dict[str, MyTool]) -> None:
        self.all_tools.update(tools)
        self.logger.info(f"Registered tools: {list(tools.keys())}")
        

    async def generate_response(self, user_input: str, conversation_history: str = "", retrieved_knowledge: List[str] = "") -> PlanningStep:
        current_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        persona = self.persona

        # 调用异步的 LLM 生成接口
        try:
            response = await self.llm.generate_response(
                user_message=user_input,
                persona=persona,
                conversation_history=conversation_history,
                knowledge="\n".join(retrieved_knowledge),
                current_time=current_time,
                tools = self.all_tools,
                use_json = False
            )
        except Exception as e:
            import traceback
            self.logger.error(f"Error during LLM response generation: {e} \n{traceback.format_exc()}")
            response = None
        if not response:
            self.logger.warning("No response generated by LLM.")
            return PlanningStep(
                action="闲聊回复",
                description="普通地回复用户的最新对话内容。",
                parameters={}
            )
        self.logger.debug(f"Planner LLM response: {response}")
        response_json = json.loads(response)
        return PlanningStep(
            action=response_json.get("action", "闲聊回复"),
            description=response_json.get("description", "普通地回复用户的最新对话内容。"),
            parameters=response_json.get("parameters", {})
        )

        

    def _init_static_variables_sync(self) -> None:
        """获取在prompt中不变的变量： persona, response_requirements, response_format (同步版本)"""
        static_variables_file = self.config.get("static_variables_file", None)
        if not static_variables_file:
            raise ValueError("static_variables_file must be provided in main_chat config")
        with open(static_variables_file, "r", encoding="utf-8") as f:
            static_vars: Dict[str, Any] = json.load(f)

        self.persona = static_vars.get("persona")